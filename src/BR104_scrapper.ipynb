{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.48.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return set(lst1).intersection(lst2)\n",
    "\n",
    "def merge_objects(object1, object2):\n",
    "    intersect_keys = intersection(object1.keys(), object2.keys())\n",
    "    if len(intersect_keys) > 0:\n",
    "        for key in intersect_keys:\n",
    "            appriori_values = object1[key] if type(object1[key]) == list else [object1[key]]\n",
    "            object2[key] = appriori_values + [object2[key]]\n",
    "    object1.update(object2)\n",
    "    return object1\n",
    "\n",
    "def get_page_data(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    base_url = \"https://www.br104.com.br/wp-json/oembed/1.0/embed?url={url}\"\n",
    "    url = base_url.format(url=url)\n",
    "    try:\n",
    "        parsed = requests.get(url).json()\n",
    "\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        article = soup.find(\"div\", class_=\"artigo\")\n",
    "        metas = soup.find_all(\"meta\")\n",
    "        meta_1 = metas[1]\n",
    "        del metas[1]\n",
    "\n",
    "        metas = [meta.attrs for meta in metas if \"property\" in meta.attrs and (\"article:\" in meta[\"property\"] or \"og:\" in meta[\"property\"]) ]\n",
    "        metas = list(map(lambda m: {m[\"property\"].replace(':','_'): m[\"content\"]} , metas))\n",
    "        metas = reduce(lambda prev, cur: merge_objects(prev, cur), metas)\n",
    "    #     metas['text'] = article.text.replace('\\n','')\n",
    "        metas['text'] = [p.text for p in article.find_all('p') if \"+ Leia tambÃ©m:\" not in p.text ]\n",
    "\n",
    "        metas[\"author_name\"] = parsed[\"author_name\"].split(\"|\")[0].strip() if \"author_name\" in parsed else \"Undefined\" \n",
    "        metas[\"author_url\"] = parsed[\"author_url\"] if \"author_url\" in parsed else \"Undefined\" \n",
    "        return metas\n",
    "    except e:\n",
    "        print(base_url)\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Page: https://www.br104.com.br/ultimas-noticias/\n",
      "Current Page: https://www.br104.com.br/ultimas-noticias/page/2\n",
      "Current Page: https://www.br104.com.br/ultimas-noticias/page/3\n",
      "Current Page: https://www.br104.com.br/ultimas-noticias/page/4\n",
      "Current Page: https://www.br104.com.br/ultimas-noticias/page/5\n",
      "Current Page: https://www.br104.com.br/ultimas-noticias/page/6\n"
     ]
    }
   ],
   "source": [
    "# # retrieving links\n",
    "links_df = pd.read_csv('links.csv')\n",
    "link_values = links_df['link'].values\n",
    "base_url = \"https://www.br104.com.br/ultimas-noticias/\" \n",
    "current_page = 0\n",
    "pages= []\n",
    "while True:\n",
    "    current_page+=1\n",
    "    url = base_url if current_page == 1 else \"{}page/{}\".format(base_url,current_page)\n",
    "    print(\"Current Page: {}\".format(url))\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    links = soup.find_all(\"a\", class_=\"titlePageCategory\")\n",
    "    hrefs = list(map(lambda link: link['href'], links))\n",
    "    if len(links) == 0:\n",
    "        break\n",
    "    \n",
    "    pages += links\n",
    "    \n",
    "    if len(intersection(hrefs,link_values)) > 0:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# creating links dataframe\n",
    "link_dataset = list(map(lambda page: {\"link\":page['href'], \"title\":page.find('h1').text.strip(), \"subtitle\":page.find(\"p\").text.strip()}, pages))\n",
    "links_df = pd.concat([links_df, pd.DataFrame(link_dataset)])\n",
    "links_df = links_df.drop_duplicates()\n",
    "links_df.to_csv('links.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a50c9610844829b855d72cbe06278a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=47.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# getting pages\n",
    "links_df = pd.read_csv(\"links.csv\")\n",
    "br104_df = pd.read_csv(\"br104.csv\")\n",
    "links = links_df['link'].values\n",
    "links_to_scrap = [link for link in links if link not in br104_df['og_url'].values]\n",
    "\n",
    "data = []\n",
    "for url in tqdm(links_to_scrap):\n",
    "    data.append(get_page_data(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "br104_df = pd.concat([br104_df, df])\n",
    "br104_df = br104_df.sort_values(by=[\"article_published_time\"], ascending=False)\n",
    "br104_df = br104_df.drop_duplicates(subset='og_url', keep=\"last\")\n",
    "br104_df.to_csv(\"br104.csv\", index= False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
